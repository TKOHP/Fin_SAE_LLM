{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    #import google.colab # type: ignore\n",
    "    #from google.colab import output\n",
    "    %pip install git+https://github.com/TKOHP/SAELens.git transformer-lens circuitsvis\n",
    "except:\n",
    "    from IPython import get_ipython # type: ignore\n",
    "    ipython = get_ipython(); assert ipython is not None\n",
    "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "    ipython.run_line_magic(\"autoreload\", \"2\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/5.55G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13697ad77097463f8296c332127d52c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import CacheActivationsRunnerConfig,CacheActivationsRunner\n",
    "from datasets import load_dataset\n",
    "dataset=load_dataset(\"Duxiaoman-DI/FinCorpus\")\n",
    "cfg=CacheActivationsRunnerConfig(\n",
    "    model_name=\"Duxiaoman-DI/XuanYuan-6B-Chat\",\n",
    "    model_class_name=\"LlamaForCausalLM\",\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",\n",
    "    context_size=512,\n",
    "    d_in=4096,\n",
    "    training_tokens=1024*30_00,\n",
    "    n_shuffles_final=100,# 存储完所有的buffer后，最终再进行n次shuffle（任意两个作shuffle）\n",
    "    shuffle_every_n_buffers= 100,# 每存储n个buffer，作shuffle\n",
    "    n_shuffles_with_last_section= 100,# 还不走\n",
    "    n_shuffles_in_entire_dir= 100,# 还不知\n",
    "    n_batches_in_buffer=8,# 和训练的一样\n",
    "    store_batch_size_prompts=8,# 和训练的一样\n",
    "    new_cached_activations_path=\"D:/project/dataset/activations/1\",\n",
    "    # ignore\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\"\n",
    ")\n",
    "a=CacheActivationsRunner(cfg,override_dataset=dataset).run()\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T07:23:46.501180300Z",
     "start_time": "2024-07-24T07:23:23.048312900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oAsZCAdJOVHw",
    "outputId": "9b75fff2-3f7e-410b-c0f6-d473f5e56c7d",
    "ExecuteTime": {
     "end_time": "2024-07-23T08:47:14.045504700Z",
     "start_time": "2024-07-23T08:46:57.539452600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\tools\\anaconda3\\envs\\llm_test\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type tinyllm to instantiate a model of type llama. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "HookedTransformer.__init__() missing 1 required positional argument: 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 26\u001B[0m\n\u001B[0;32m     24\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path)\n\u001B[0;32m     25\u001B[0m model \u001B[38;5;241m=\u001B[39m LlamaForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name_or_path, device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,offload_folder\u001B[38;5;241m=\u001B[39moffload_folder)\n\u001B[1;32m---> 26\u001B[0m model\u001B[38;5;241m=\u001B[39m\u001B[43mHookedTransformerModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m cfg \u001B[38;5;241m=\u001B[39m LanguageModelSAERunnerConfig(\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# Data Generating Function (Model + Training Distibuion)\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpythia-14m\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     86\u001B[0m \n\u001B[0;32m     87\u001B[0m )\n\u001B[0;32m     88\u001B[0m model\u001B[38;5;241m.\u001B[39mcfg\u001B[38;5;241m=\u001B[39mcfg\n",
      "File \u001B[1;32mD:\\project\\LLM\\myproject\\load_own_model.py:11\u001B[0m, in \u001B[0;36mHookedTransformerModel.__init__\u001B[1;34m(self, model)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, model):\n\u001B[1;32m---> 11\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mHookedTransformerModel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m model\n",
      "\u001B[1;31mTypeError\u001B[0m: HookedTransformer.__init__() missing 1 required positional argument: 'cfg'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from sae_lens import LanguageModelSAERunnerConfig, SAETrainingRunner\n",
    "device = \"cuda\"\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "total_training_steps = 30_00  # probably we should do more\n",
    "batch_size = 1024\n",
    "total_training_tokens = total_training_steps * batch_size\n",
    "\n",
    "lr_warm_up_steps = 0\n",
    "lr_decay_steps = total_training_steps // 5  # 20% of training\n",
    "l1_warm_up_steps = total_training_steps // 20  # 5% of training\n",
    "\n",
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    ######## 如果是存储好activations的话，下面配置忽略\n",
    "    mmodel_name=\"Duxiaoman-DI/XuanYuan-6B-Chat\",\n",
    "    model_class_name=\"LlamaForCausalLM\",\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",\n",
    "    #######\n",
    "    hook_name=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_layer=0,  # Only one layer in the model.\n",
    "    d_in=4096,  # the width of the mlp output.\n",
    "    is_dataset_tokenized=True,\n",
    "    streaming=False,  # we could pre-download the token dataset if it was small.\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=2**3,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n",
    "    normalize_sae_decoder=False,\n",
    "    scale_sparsity_penalty_by_decoder_norm=True,\n",
    "    decoder_heuristic_init=True,\n",
    "    init_encoder_as_decoder_transpose=True,\n",
    "    normalize_activations=\"expected_average_only_in\",\n",
    "    # normalize_activations=\"none\",\n",
    "    # Training Parameters\n",
    "    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n",
    "    adam_beta2=0.999,\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n",
    "    l1_coefficient=5,  # will control how sparse the feature activations are\n",
    "    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size_tokens=batch_size,\n",
    "    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=8,  # controls how many activations we store / shuffle.\n",
    "    # n_batches_in_buffer=8,  # controls how many activations we store / shuffle.\n",
    "    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    # store_batch_size_prompts=32,\n",
    "    store_batch_size_prompts=8,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,  # we don't use ghost grads anymore.\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=30,\n",
    "    eval_every_n_wandb_logs=20,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    # act_store_device=\"cpu\",\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=\"float32\",\n",
    "    # store_activations\n",
    "    use_cached_activations=True,\n",
    "    cached_activations_path=\"D:/project/dataset/activations/1\"\n",
    ")\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder = SAETrainingRunner(cfg).run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "https://github.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb",
     "timestamp": 1721283432729
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
